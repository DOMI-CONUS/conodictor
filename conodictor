#!/usr/bin/env python3

import argparse
from Bio import SearchIO
from Bio.Seq import reverse_complement, translate
from collections import Counter, defaultdict
import csv
from datetime import datetime
from distutils.spawn import find_executable
from functools import reduce
import gzip
from heapq import nsmallest
from matplotlib import pyplot as plt
import mmh3
import numpy as np
from operator import mul
import os
import pandas as pd
import pathlib
import platform
import pyfastx
import re
import shutil
import subprocess
import sys
import warnings

AUTHOR = "Anicet Ebou <anicet.ebou@gmail.com>"
URL = "https://github.com/koualab/conodictor.git"
VERSION = "2.1.4"

# Define start time----------------------------------------------------------
startime = datetime.now()
helptime = datetime.now().strftime("%a, %d %b %Y %H:%M:%S")

# Define command-line arguments----------------------------------------------
parser = argparse.ArgumentParser(
    prog="conodictor",
    formatter_class=argparse.RawDescriptionHelpFormatter,
    usage="conodictor [options] seqs.fa.gz",
    epilog=f"Version:   {VERSION}\nLicence:   GPL-3\n"
    + f"Homepage:  {URL}\nAuthor:    {AUTHOR}\nLast Run:  {helptime}.",
)

parser.add_argument("seqs", help="Specify input fasta file.")
parser.add_argument(
    "--out",
    type=pathlib.Path,
    default="ConoDictor",
    help="Specify output folder.",
)
parser.add_argument(
    "--mlen",
    type=int,
    help="Minimum sequence length to consider for prediction.",
)
parser.add_argument(
    "--ndup",
    type=int,
    help="Minimum number of sequence occurence"
    + " for a sequence to be considered.",
)
parser.add_argument(
    "--all",
    action="store_true",
    help="Display sequence without hits in output. Default: False.",
)
parser.add_argument(
    "--cpus",
    type=int,
    default=1,
    help="Specify the number of threads. Default: 1.",
)
parser.add_argument(
    "--force",
    action="store_true",
    help="Force re-use output directory. Default: Off.",
)
parser.add_argument(
    "--quiet", action="store_true", help="Decrease program verbosity"
)
parser.add_argument(
    "--debug", action="store_true", help="Activate debug mode"
)
args = parser.parse_args()


def main():
    # Handling db directory path specification-------------------------------
    try:
        dbdir = os.environ["CONODB"]
    except KeyError:
        print(
            "conodictor: error: Models for predictions not found in $PATH.",
            file=sys.stderr,
        )
        print(
            "Please set CONODB environment variable to the path "
            + "where models are stored.",
            file=sys.stderr,
        )
        print(
            "Visit https://github.com/koualab/conodictor for more",
            file=sys.stderr,
        )
        sys.exit(1)

    # Handling output directory creation-------------------------------------
    if os.path.isdir(args.out):
        if args.force:
            msg(f"Reusing outdir {args.out}")
            shutil.rmtree(args.out)
            os.mkdir(args.out)
        else:
            print("usage: conodictor [options] seqs.fa.gz", file=sys.stderr)
            print(
                f"conodictor: error: Your choosen output folder '{args.out}'"
                + " already exist!. Please change it using --out option"
                + " or use --force to reuse it.",
                file=sys.stderr,
            )
            sys.exit(1)
    else:
        msg(f"Creating output directory {args.out}")
        os.mkdir(args.out)

    # Get current user name
    try:
        user = os.environ["USER"]
    except KeyError:
        user = "not telling me who you are"

    # Start program ---------------------------------------------------------
    msg("-----------------------------------------------")
    msg("conodictor Copyright (C) 2021 Anicet Ebou")
    msg("This program comes with ABSOLUTELY NO WARRANTY;")
    msg("This is free software, and you are welcome to ")
    msg("redistribute it under certain conditions.")
    msg("-----------------------------------------------")
    msg(f"This is conodictor {VERSION}")
    msg(f"Written by {AUTHOR}")
    msg(f"Available at {URL}")
    msg(f"Localtime is {datetime.now().strftime('%H:%M:%S')}")
    msg(f"You are {user}")
    msg(f"Operating system is {platform.system()}")

    # Handling number of cpus -----------------------------------------------
    cpus = args.cpus
    msg(f"System has {os.cpu_count()} cores")

    if args.cpus == 0:
        cpus = os.cpu_count()
    elif args.cpus > os.cpu_count():
        msg(
            f"Option --cpus asked for {args.cpus} cores,"
            + f" but system has only {os.cpu_count()}."
        )
        cpus = os.cpu_count()
    msg(f"We will use maximum of {cpus} cores.")

    # Verify presence of needed tools ---------------------------------------
    needed_tools = ("hmmsearch", "pfscanV3")

    for tool in needed_tools:
        if find_executable(tool) is not None:
            msg(f"Found {tool}")
        else:
            print_install_tool(tool)

    # Getting version of tools ----------------------------------------------
    sub_hmmsearch = subprocess.run(["hmmsearch", "-h"], capture_output=True)
    hmmsearch_match = re.findall(
        r"# HMMER\s+(\d+\.\d+)", sub_hmmsearch.stdout.decode("utf-8")
    )

    sub_pfscan = subprocess.run(["pfscanV3", "-h"], capture_output=True)
    pfscan_match = re.findall(
        r"Version\s+(\d+\.\d+\.\d+)", sub_pfscan.stdout.decode("utf-8")
    )

    # Check that version-----------------------------------------------------
    if hmmsearch_match[0] and float(hmmsearch_match[0]) > 3:
        hmmsearch_version = hmmsearch_match[0]
    elif hmmsearch_match[0] and float(hmmsearch_match[0]) < 3:
        raise ValueError(
            "hmmsearch installed is below 3.0 version, please upgrade"
            + " at https://hmmer3.org."
        )

    else:
        raise ValueError(
            "Cannot parse HMMER version. Please check it's correctly"
            + " installed. See https://hmmer3.org."
        )

    # Input sequence file manipulation---------------------------------------

    # Open fasta file (build file index)
    infa = pyfastx.Fasta(args.seqs)

    # Test if file type is accepted
    if isdnaorproteins(infa[1].seq) in ["DNA", "protein"]:
        pass
    else:
        msg(
            "Your file is not a DNA or protein file, please provide a DNA or"
            + " protein fasta file"
        )
        sys.exit(1)

    # Test if file is gziped and translate
    if infa.is_gzip:
        # Decompress file
        msg("Your file is gzip compressed. Decompressing it.")
        with gzip.open(args.seqs, "r") as seqh:
            with open(
                pathlib.Path(args.out, pathlib.Path(args.seqs).stem), "wb"
            ) as seqo:
                shutil.copyfileobj(seqh, seqo)
            seqo.close()
        msg("Decompression done.")

        # Read decompressed file
        ingzfa = pyfastx.Fasta(
            str(pathlib.Path(args.out, pathlib.Path(args.seqs).stem))
        )

        # Test if alphabet is DNA, or protein and translate or not
        if isdnaorproteins(ingzfa[1].seq) == "DNA":
            msg("You provided DNA fasta file")
            msg("Translating input sequences")
            do_translation(
                str(pathlib.Path(args.out, pathlib.Path(args.seqs).stem)),
                str(pathlib.Path(args.out, pathlib.Path(args.seqs).stem)),
            )
            msg("Translation done!")
            inpath = pathlib.Path(
                args.out, f"{pathlib.Path(args.seqs).stem}_proteins.fa"
            )
        elif isdnaorproteins(ingzfa[1].seq) == "protein":
            msg("You provided protein fasta file")
            inpath = pathlib.Path(args.out, pathlib.Path(args.seqs).stem)

    # If fasta file is not gzipped
    elif not infa.is_gzip:
        msg("Your file is not gzip compressed")
        if isdnaorproteins(infa[1].seq) == "DNA":
            msg("You provided DNA fasta file")
            msg("Translating input sequences")
            do_translation(
                str(pathlib.Path(args.seqs)),
                str(pathlib.Path(args.out, pathlib.Path(args.seqs).stem)),
            )
            msg("Translation done!")
            inpath = pathlib.Path(
                args.out, f"{pathlib.Path(args.seqs).stem}_proteins.fa"
            )
        elif isdnaorproteins(infa[1].seq) == "protein":
            msg("You provided protein fasta file")
            inpath = args.seqs

    # Build sequence index and get list of keys -----------------------------
    infile = pyfastx.Fasta(str(inpath))
    seqids = infile.keys()

    # If --mlen is specified, filter out sequence with len < mlen.
    if args.mlen:
        if infile.count(args.mlen) == 0:
            msg(f"Input file contains 0 sequences with length >= {args.mlen}")
            msg("No sequence will be predicted. Conodictor is stopping...")
            sys.exit(1)
        else:
            msg(
                f"Input file contains {infile.count(args.mlen)}"
                + f" sequences with length >= {args.mlen}"
            )
            seqids = seqids.filter(seqids >= args.mlen)

    # If --ndup is specified, get sequence ids of duplicate sequence.
    if args.ndup:
        seqids = get_dup_seqs(infile, seqids, args.ndup)
        if args.mlen is None:
            msg(
                f"Input file contains {len(seqids)}"
                + f" sequences with at least {args.ndup} occurences."
                + " Only these sequences will be used for prediction."
            )
        else:
            msg(
                f"And from them we have {len(seqids)} sequences"
                + f" with at least {args.ndup} occurences."
                + " Only these sequences will be used for prediction."
            )
    # Create a fasta file of sequence after filtering
    if args.ndup is not None or args.mlen is not None:
        with open(pathlib.Path(args.out, "filtfa.fa"), "w") as fih:
            for kid in seqids:
                fih.write(f">{infile[kid].description}\n{infile[kid].seq}\n")
        fih.close()

        # Use the filtered file as input of further commands
        final_file = pathlib.Path(args.out, "filtfa.fa")
    else:
        # Use the unfiltered file as input of further commands
        final_file = inpath

    # HMMs-------------------------------------------------------------------
    msg("Running HMM prediction")
    msg(f"Using hmmsearch v{hmmsearch_version}")
    subprocess.run(
        [
            "hmmsearch",
            "--cpu",
            str(cpus),
            "-E",
            "0.1",
            "--noali",
            "-o",
            pathlib.Path(args.out, "out.hmmer"),
            pathlib.Path(dbdir, "conodictor.hmm"),
            final_file,
        ]
    )

    hmmdict = defaultdict(lambda: defaultdict(list))

    with open(pathlib.Path(args.out, "out.hmmer")) as hmmfile:
        for record in SearchIO.parse(hmmfile, "hmmer3-text"):
            hits = record.hits
            for hit in hits:
                hmmdict[hit.id][record.id.split("_")[1]].append(hit.evalue)
    hmmfile.close()

    hmmscore = hmm_threshold(hmmdict)
    hmmfam = get_hmm_fam(hmmscore)

    msg("Done with HMM prediction")

    # PSSMs------------------------------------------------------------------
    msg("Running PSSM prediction")
    msg(f"Using pfscan v{pfscan_match[0]}")
    pssm_run = subprocess.run(
        [
            "pfscanV3",
            "--nthreads",
            str(cpus),
            "-o",
            "7",
            pathlib.Path(dbdir, "conodictor.pssm"),
            "-f",
            final_file,
        ],
        capture_output=True,
    )

    with open(pathlib.Path(args.out, "out.pssm"), "w") as po:
        po.write(pssm_run.stdout.decode("utf-8"))
    po.close()

    pssmdict = defaultdict(list)

    with open(pathlib.Path(args.out, "out.pssm")) as pssmfile:
        rd = csv.reader(pssmfile, delimiter="\t")
        for row in rd:
            pssmdict[row[3]].append((row[0].split("|")[0]).split("_")[1])
    pssmfile.close()

    pssmfam = get_pssm_fam(pssmdict)

    msg("Done with PSSM predictions")

    # Writing output---------------------------------------------------------
    msg("Writing output")
    finalfam = defaultdict(list)

    for sid in seqids:
        if sid in hmmfam and sid in pssmfam:
            finalfam[sid].extend(
                [
                    hmmfam[sid],
                    pssmfam[sid],
                    definitive_prediction(hmmfam[sid], pssmfam[sid]),
                ]
            )
        elif sid in hmmfam and sid not in pssmfam:
            finalfam[sid].extend([hmmfam[sid], "UNKNOWN", hmmfam[sid]])
        elif sid in pssmfam and sid not in hmmfam:
            finalfam[sid].extend(["UNKNOWN", pssmfam[sid], pssmfam[sid]])
        else:
            finalfam[sid].extend(["UNKOWN", "UNKOWN", "UNKNOWN"])

    outfile = open(pathlib.Path(args.out, "summary.txt"), "a")
    outfile.write(
        "sequence\tlength\tnum_cysteines\toccurence\t"
        + "hmm_pred\tpssm_pred\tdefinitive_pred\n"
    )
    seq_occur = get_seq_occur(seqids, infile)
    occur = Counter(seq_occur.values())

    if not args.all:
        uniq_final = {
            k: v
            for k, v in finalfam.items()
            if bool(set(v).intersection(["UNKNOWN", "UNKNOWN", "UNKNOWN"]))
            is False
        }
        for uk, uv in uniq_final.items():
            outfile.write(
                f"{uk}\t"  # sequence id
                + f"{get_stats(uk, infile)[0]}\t"  # sequence length
                + f"{get_stats(uk, infile)[1]}\t"  # sequence num of cysteines
                + f"{occur[seq_occur[uk]]}\t"  # seq occ
                + f"{uv[0]}\t"  # sequence HMM prediction
                + f"{uv[1]}\t"  # sequence PSSM prediction
                + f"{uv[2]}\n"  # sequence ConoDictor prediction
            )
        outfile.close()
    else:
        for k, v in finalfam.items():
            outfile.write(
                f"{k}\t"
                + f"{get_stats(k, infile)[0]}\t"
                + f"{get_stats(k, infile)[1]}\t"
                + f"{occur[seq_occur[k]]}t"
                + f"{v[0]}\t"
                + f"{v[1]}\t"
                + f"{v[2]}\n"
            )
        outfile.close()
    msg("Done with writing output.")

    # Finishing -------------------------------------------------------------
    # Cleaning around ...
    os.remove(pathlib.Path(args.out, "out.hmmer"))
    os.remove(pathlib.Path(args.out, "out.pssm"))
    os.remove(pathlib.Path(f"{args.seqs}.fxi"))
    os.remove(pathlib.Path(f"{inpath}.fxi"))
    if args.mlen or args.ndup:
        os.remove(pathlib.Path(args.out, "filtfa.fa"))

    msg("Classification finished successfully.")
    msg("Creating donut plot")
    donut_graph()
    msg("Done creating donut plot")
    msg(f"Check {args.out} folder for results")
    endtime = datetime.now()
    walltime = endtime - startime
    msg(f"Walltime used (hh:mm:ss.ms): {walltime}")
    if len(seqids) % 2:
        msg("Nice to have you. Share, enjoy and come back!")
    else:
        msg("Thanks you, come again.")


# Functions -----------------------------------------------------------------
def get_seq_occur(ids, infile):
    """
    get_seq_occur return a dict of seqid: Murmurhashed sequences at 128
    bits.

    :ids: list of sequence ids to consider
    :infile: file to use to retrieve sequences
    """
    # The returned hash will be used to count for sequence occurence.
    # I opt for this option as the hash will be smaller than a hash
    # of pure sequences stored in memory in case of a big file.

    seqmmh = defaultdict()
    for id in ids:
        seqmmh[id] = mmh3.hash128(infile[id].seq, 47)
    return seqmmh


def get_stats(id, infile):
    """
    get_stats return sequence length and number of cysteines in a sequences
    for a sequence id.

    :id: Input sequence id list.
    :infile: Fasta file to use to retrieve sequence.
    """
    stats = []

    # Sequence length
    stats.append(len(infile[id]))
    # Number of cysteines in sequence
    stats.append(infile[id].seq.count("C"))

    return stats


def get_dup_seqs(infile, idslist, mnoc):
    """
    get_dup_seqs search provided fasta file for duplicate sequence.
    Return sequence ids of duplicate sequences.

    :infile: Input fasta file to use for search
    :idslist: Sequence ids list to consider
    :mnoc: Minimum number of occurence wanted
    """

    dupid = []
    flipped = defaultdict(set)
    seqdict = defaultdict()
    for id in idslist:
        seqdict[id] = infile[id].seq

    flipped = _flip_dict(seqdict)

    for v in flipped.values():
        if len(v) >= mnoc:
            dupid.extend(v)

    return dupid


def _flip_dict(mydict):
    """
    Return a flipped dict of input dict
    """

    flipped_ = defaultdict(list)
    for k, v in mydict.items():
        if v not in flipped_:
            flipped_[v] = [k]
        else:
            flipped_[v].append(k)

    return flipped_


def donut_graph():
    """
    donut_graph make a donut graph from outputed stats of
    predicted sequences.
    """

    data = pd.read_table(pathlib.Path(args.out, "summary.txt"))
    plot_data = data[data.columns[6]].tolist()
    dtc = Counter(plot_data)
    labels = [
        f"{k1}: {v1}"
        for k1, v1 in dtc.items()
        if not k1.startswith("CONFLICT")
    ]
    values = [x for k2, x in dtc.items() if not k2.startswith("CONFLICT")]

    # White circle
    _, ax = plt.subplots(figsize=(8, 5), subplot_kw=dict(aspect="equal"))
    wedges, _ = ax.pie(
        np.array(values).ravel(), wedgeprops=dict(width=0.5), startangle=-40
    )
    ax.legend(
        wedges, labels, loc="center left", bbox_to_anchor=(1, 0, 0.5, 1)
    )
    ax.set_title("ConoDictor Predictions")
    plt.savefig(
        pathlib.Path(args.out, "superfamilies_distribution.png"), dpi=300
    )


def definitive_prediction(hmmclass, pssmclass):
    """
    definitive_prediction gives definitive classification by
    combining HMM and PSSM classification.

    :hmmclass: HMM predicted family, required (string)
    :pssmclass: PSSM predicted family, required (string)
    """

    deffam = None

    if hmmclass == pssmclass:
        deffam = hmmclass
    elif "CONFLICT" in pssmclass and "CONFLICT" in hmmclass:
        fams_pssm = re.search("(?<=CONFLICT)(.*)and(.*)", pssmclass)
        fams_hmm = re.search("(?<=CONFLICT)(.*)and(.*)", hmmclass)
        deffam = f"CONFLICT {fams_pssm.group(1)}, {fams_pssm.group(2)},"
        +f" {fams_hmm.group(1)}, and {fams_hmm.group(2)}"
    elif "CONFLICT" in pssmclass and "CONFLICT" not in hmmclass:
        deffam = hmmclass
    elif "CONFLICT" in hmmclass and "CONFLICT" not in pssmclass:
        deffam = pssmclass
    elif pssmclass != hmmclass:
        deffam = f"CONFLICT {hmmclass} and {pssmclass}"

    return deffam


def isdnaorproteins(s):
    """
    isdnaorproteins test if input sequence is DNA or proteins.

    :s: input sequence
    """

    dna = "ATCG"
    prot = "ABCDEFGHIKLMNPQRSTVWXYZ"
    stype = ""

    if all(i in dna for i in s):
        stype = "DNA"
    elif all(i in prot for i in s):
        stype = "protein"
    else:
        stype = "unknown"

    return stype


def get_pssm_fam(mdict):
    """
    get_pssm_fam return the family with the highest number of
    occurence in PSSM profile match recorded as list for each
    sequence id.

    >>> my_dict = {ID1: ['A', 'A', 'B', 'M'], ID2: ['M', 'P', 'O1', 'O1']}
    >>> get_pssm_fam(my_dict)
    {ID1: 'A', ID2: 'O1'}

    :mdict: Dictionnary, required (dict)
    """

    fam = ""
    pssmfam = {}
    for key in mdict.keys():
        x = Counter(mdict[key])
        # Take the top 2 item with highest count in list
        possible_fam = x.most_common(2)

        if len(possible_fam) == 1:
            fam = possible_fam[0][0]
        elif len(possible_fam) > 1:
            if possible_fam[0][1] == possible_fam[1][1]:
                fam = (
                    f"CONFLICT {possible_fam[0][0]} and {possible_fam[1][0]}"
                )
            elif possible_fam[0][1] > possible_fam[1][1]:
                fam = possible_fam[0][0]
            else:
                fam = possible_fam[1][0]

        pssmfam[key] = fam

    return pssmfam


def hmm_threshold(mdict):
    """
    hmm_threshold calculate evalue by family for each sequence
    and return a dict with the evalue for each family.

    :mdict: Dictionnary, required (dict)
    """

    score = defaultdict(dict)
    for key in mdict.keys():
        for k, v in mdict[key].items():
            score[key][k] = reduce(mul, v, 1)

    return score


def get_hmm_fam(mdict):
    """
    get_hmm_fam get sequence family from hmm dictionnary.

    :mdict: Dictionnary of evalues by families.
    """

    conofam = ""
    seqfam = {}
    for key in mdict.keys():
        two_smallest = nsmallest(2, mdict[key].values())

        if len(two_smallest) == 1:
            conofam = next(iter(mdict[key]))
        elif two_smallest[0] * 100 != two_smallest[1]:
            conofam = list(mdict[key].keys())[
                list(mdict[key].values()).index(two_smallest[0])
            ]
        elif two_smallest[0] * 100 == two_smallest[1]:
            fam1 = list(mdict[key].keys())[
                list(mdict[key].values()).index(two_smallest[0])
            ]
            fam2 = list(mdict[key].keys())[
                list(mdict[key].values()).index(two_smallest[1])
            ]
            conofam = f"CONFLICT {fam1} and {fam2}"

        seqfam[key] = conofam

    return seqfam


def print_install_tool(tool):
    """
    print_install_tool print useful installation
    instruction for required tools.
    """

    if tool == "hmmsearch":
        msg(f"{tool} not found. Please visit https://hmmer3.org.")
    elif tool == "pfscanV3":
        msg(
            f"{tool} not found. Please visit"
            + "https://github.com/sib-swiss/pftools3."
        )

    sys.exit(1)


def msg(text):
    """
    msg produce nice message and info output on terminal.

    :text: Message to print to STDOUT.
    """

    t = datetime.now().strftime("%H:%M:%S")
    line = f"[{t}] {text}"
    if not args.quiet:
        print(line, file=sys.stderr)


def _translate_seq(seq):
    """
    _translate_seq translate DNA sequence to proteins in the six frames.

    :seq: DNA sequence to translate.
    """

    seqlist = []
    # frame 1
    seqlist.append(translate(seq))
    # frame 2
    seqlist.append(translate(seq[1:]))
    # frame 3
    seqlist.append(translate(seq[2:]))
    # frame 4
    seqlist.append(translate(reverse_complement(seq)))
    # frame 5
    seqlist.append(translate(reverse_complement(seq)[1:]))
    # frame 6
    seqlist.append(translate(reverse_complement(seq)[2:]))

    return seqlist


def do_translation(infile, outfile, sw=60):
    """
    do_translation translate a DNA fasta file into proteins
    fasta file.

    :infile: Input DNA fasta file.
    :outfile: Output file.
    :sw: Sequence width. Default: 60.
    """

    seqin = pyfastx.Fasta(infile)
    with open(pathlib.Path(f"{outfile}_proteins.fa"), "w") as protfile:
        for sequence in seqin:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                protseq = _translate_seq(sequence.seq)
                for idx, frame in enumerate(protseq):
                    # Rule E203 from flacke8 check for extraneous whitespace
                    # before a colon. But black follow PEP8 rules.
                    # A PR is open to resolve this issue:
                    # https://github.com/PyCQA/pycodestyle/pull/914
                    seq_letters = [
                        frame[i : i + sw]  # noqa: E203
                        for i in range(0, len(frame), sw)
                    ]
                    nl = "\n"
                    protfile.write(
                        f">{sequence.name}_frame={idx + 1}\n"
                        + f"{nl.join(map(str, seq_letters))}\n"
                    )


def exception_handler(
    exception_type, exception, traceback, debug_hook=sys.excepthook
):
    """
    exception_handler remove default debug info and traceback
    from python output on command line. Use program --debug
    option to re-enable default behaviour.
    """

    if args.debug:
        debug_hook(exception_type, exception, traceback)
    else:
        msg(f"{exception_type.__name__}, {exception}")


sys.excepthook = exception_handler


if __name__ == "__main__":
    main()
