#!/usr/bin/env python3

import argparse
from Bio import SearchIO
from Bio.Seq import reverse_complement, translate
from collections import Counter, defaultdict
import csv
from datetime import datetime
from decimal import Decimal
from distutils.spawn import find_executable
from functools import reduce
import gzip
from heapq import nsmallest
from matplotlib import pyplot as plt
import numpy as np
from operator import mul
import os
import pandas as pd
import pathlib
import platform
import pyfastx
import re
import shutil
import subprocess
import sys
import warnings

AUTHOR = "Anicet Ebou, Dominique Koua"
URL = "https://github.com/koualab/conodictor.git"
VERSION = "2.2.3"

# Define start time----------------------------------------------------------
startime = datetime.now()
helptime = datetime.now().strftime("%a, %d %b %Y %H:%M:%S")

# Define command-line arguments----------------------------------------------
parser = argparse.ArgumentParser(
    prog="conodictor",
    formatter_class=argparse.RawDescriptionHelpFormatter,
    usage="conodictor [options] seqs.fa.gz",
    epilog=f"Version:   {VERSION}\nLicence:   GPL-3\n"
    + f"Homepage:  {URL}\nAuthors:   {AUTHOR}\nLast Run:  {helptime}.",
)

parser.add_argument("seqs", help="Specify input fasta file.")
parser.add_argument(
    "--out",
    type=pathlib.Path,
    default="ConoDictor",
    help="Specify output folder.",
)
parser.add_argument(
    "--mlen",
    type=int,
    help="Set the minimum length of the sequence to be considered as a match",
)
parser.add_argument(
    "--ndup",
    type=int,
    help="Minimum sequence occurence of a sequence to be considered",
)
parser.add_argument(
    "--faa",
    action="store_true",
    help="Create a fasta file of matched sequences. Default: False.",
)
parser.add_argument(
    "--nofilter",
    action="store_true",
    help="Desactivate the signal and pro-regions filter. Default: False.",
)
parser.add_argument(
    "--all",
    action="store_true",
    help="Display sequence without hits in output. Default: False.",
)
parser.add_argument(
    "--cpus",
    type=int,
    default=1,
    help="Specify the number of threads. Default: 1.",
)
parser.add_argument(
    "--force",
    action="store_true",
    help="Force re-use output directory. Default: Off.",
)
parser.add_argument(
    "--quiet", action="store_true", help="Decrease program verbosity"
)
parser.add_argument(
    "--debug", action="store_true", help="Activate debug mode"
)
args = parser.parse_args()


def main():
    # Handling db directory path specification-------------------------------
    try:
        dbdir = os.environ["CONODB"]
    except KeyError:
        print(
            "conodictor: error: Models for predictions not found in $PATH.",
            file=sys.stderr,
        )
        print(
            "Please set CONODB environment variable to the path "
            + "where models are stored.",
            file=sys.stderr,
        )
        print(
            "Visit https://github.com/koualab/conodictor for more",
            file=sys.stderr,
        )
        sys.exit(1)

    # Handling output directory creation-------------------------------------
    if os.path.isdir(args.out):
        if args.force:
            msg(f"Reusing outdir {args.out}")
            shutil.rmtree(args.out)
            os.mkdir(args.out)
        else:
            print("usage: conodictor [options] seqs.fa.gz", file=sys.stderr)
            print(
                f"conodictor: error: Your choosen output folder '{args.out}'"
                + " already exist!. Please change it using --out option"
                + " or use --force to reuse it.",
                file=sys.stderr,
            )
            sys.exit(1)
    else:
        msg(f"Creating output directory {args.out}")
        os.mkdir(args.out)

    # Get current user name
    try:
        user = os.environ["USER"]
    except KeyError:
        user = "not telling me who you are"

    # Start program ---------------------------------------------------------
    msg("-----------------------------------------------")
    msg("conodictor Copyright (C) 2020 - 2021 Koualab")
    msg("This program comes with ABSOLUTELY NO WARRANTY;")
    msg("This is free software, and you are welcome to ")
    msg("redistribute it under certain conditions.")
    msg("-----------------------------------------------")
    msg(f"This is conodictor {VERSION}")
    msg(f"Written by {AUTHOR}")
    msg(f"Available at {URL}")
    msg(f"Localtime is {datetime.now().strftime('%H:%M:%S')}")
    msg(f"You are {user}")
    msg(f"Operating system is {platform.system()}")

    # Handling number of cpus -----------------------------------------------
    cpus = args.cpus
    msg(f"System has {os.cpu_count()} cores")

    if args.cpus == 0:
        cpus = os.cpu_count()
    elif args.cpus > os.cpu_count():
        msg(
            f"Option --cpus asked for {args.cpus} cores,"
            + f" but system has only {os.cpu_count()}."
        )
        cpus = os.cpu_count()
    msg(f"We will use maximum of {cpus} cores.")

    # Verify presence of needed tools ---------------------------------------
    needed_tools = ("hmmsearch", "pfscanV3")

    for tool in needed_tools:
        if find_executable(tool) is not None:
            msg(f"Found {tool}")
        else:
            print_install_tool(tool)

    # Getting version of tools ----------------------------------------------
    sub_hmmsearch = subprocess.run(["hmmsearch", "-h"], capture_output=True)
    hmmsearch_match = re.findall(
        r"# HMMER\s+(\d+\.\d+)", sub_hmmsearch.stdout.decode("utf-8")
    )

    sub_pfscan = subprocess.run(["pfscanV3", "-h"], capture_output=True)
    pfscan_match = re.findall(
        r"Version\s+(\d+\.\d+\.\d+)", sub_pfscan.stdout.decode("utf-8")
    )

    # Check that version-----------------------------------------------------
    if hmmsearch_match[0] and float(hmmsearch_match[0]) > 3:
        hmmsearch_version = hmmsearch_match[0]
    elif hmmsearch_match[0] and float(hmmsearch_match[0]) < 3:
        raise ValueError(
            "hmmsearch installed is below 3.0 version, please upgrade"
            + " at https://hmmer3.org."
        )

    else:
        raise ValueError(
            "Cannot parse HMMER version. Please check it's correctly"
            + " installed. See https://hmmer3.org."
        )

    # Input sequence file manipulation---------------------------------------

    # Open fasta file (build file index)
    infa = pyfastx.Fasta(args.seqs)

    # Test if file type is accepted
    if isdnaorproteins(infa[1].seq) in ["DNA", "protein"]:
        pass
    else:
        msg(
            "Your file is not a DNA or protein file, please provide a DNA or"
            + " protein fasta file"
        )
        sys.exit(1)

    # Test if file is gziped and translate
    if infa.is_gzip:
        # Decompress file
        msg("Your file is gzip compressed. Decompressing it.")
        with gzip.open(args.seqs, "r") as seqh:
            with open(
                pathlib.Path(args.out, pathlib.Path(args.seqs).stem), "wb"
            ) as seqo:
                shutil.copyfileobj(seqh, seqo)
            seqo.close()
        msg("Decompression done.")

        # Read decompressed file
        ingzfa = pyfastx.Fasta(
            str(pathlib.Path(args.out, pathlib.Path(args.seqs).stem))
        )

        # Test if alphabet is DNA, or protein and translate or not
        if isdnaorproteins(ingzfa[1].seq) == "DNA":
            msg("You provided DNA fasta file")
            msg("Translating input sequences")
            do_translation(
                str(pathlib.Path(args.out, pathlib.Path(args.seqs).stem)),
                str(pathlib.Path(args.out, pathlib.Path(args.seqs).stem)),
            )
            msg("Translation done!")
            inpath = pathlib.Path(
                args.out, f"{pathlib.Path(args.seqs).stem}_proteins.fa"
            )
        elif isdnaorproteins(ingzfa[1].seq) == "protein":
            msg("You provided protein fasta file")
            inpath = pathlib.Path(args.out, pathlib.Path(args.seqs).stem)

    # If fasta file is not gzipped
    elif not infa.is_gzip:
        msg("Your file is not gzip compressed")
        if isdnaorproteins(infa[1].seq) == "DNA":
            msg("You provided DNA fasta file")
            msg("Translating input sequences")
            do_translation(
                str(pathlib.Path(args.seqs)),
                str(pathlib.Path(args.out, pathlib.Path(args.seqs).stem)),
            )
            msg("Translation done!")
            inpath = pathlib.Path(
                args.out, f"{pathlib.Path(args.seqs).stem}_allpep.fa"
            )
        elif isdnaorproteins(infa[1].seq) == "protein":
            msg("You provided protein fasta file")
            inpath = args.seqs

    # Build sequence index and get list of keys -----------------------------
    infile = pyfastx.Fasta(str(inpath))
    seqids = infile.keys()

    # If --mlen is specified, filter out sequence with len < mlen
    if args.mlen:
        nb_occur = infile.count(args.mlen)
        if nb_occur == 0:
            msg(
                "Input file contains 0 sequences with "
                + f"length >= {args.mlen} bp"
            )
            msg("No sequence will be predicted. Conodictor is stopping...")
            sys.exit(1)
        else:
            msg(
                f"Input file contains {nb_occur}"
                + f" sequences with length >= {args.mlen} bp"
            )
            seqids = seqids.filter(seqids >= args.mlen)

    # If --ndup is specified, get sequence ids of duplicate sequence
    dupdata = {}
    if args.ndup:
        dupdata = get_dup_seqs(infile, seqids, args.ndup)
        if args.mlen is None:
            msg(
                f"Input file contains {len(dupdata)}"
                + f" sequences with at least {args.ndup} occurences."
                + " Only these sequences will be used for prediction."
            )
        else:
            msg(
                f"And from them we have {len(dupdata)} sequences"
                + f" with at least {args.ndup} occurences."
                + " Only these sequences will be used for prediction."
            )
    # Create a fasta file of sequence after filtering
    if args.ndup is not None or args.mlen is not None:
        with open(pathlib.Path(args.out, "filtfa.fa"), "w") as fih:
            for kid in dupdata.keys():
                fih.write(f">{infile[kid].description}\n{infile[kid].seq}\n")
        fih.close()

        # Use the filtered file as input of further commands
        final_file = pathlib.Path(args.out, "filtfa.fa")
    else:
        # Use the unfiltered file as input of further commands
        final_file = inpath

    # HMMs-------------------------------------------------------------------
    msg(f"Running HMM prediction using hmmsearch v{hmmsearch_version}")

    # Run hmmsearch
    subprocess.run(
        [
            "hmmsearch",
            "--cpu",
            str(cpus),
            "-E",
            "0.1",
            "--noali",
            "-o",
            pathlib.Path(args.out, "out.hmmer"),
            pathlib.Path(dbdir, "conodictor.hmm"),
            final_file,
        ]
    )

    # Create two dict:
    #  - one to filter out grouped match without MAT profile
    #  - second to create dict for classification based on kept sequences
    hmmverif = defaultdict(lambda: defaultdict(list))
    hmmdict = defaultdict(lambda: defaultdict(list))

    # First iteration over output file for filtering
    with open(pathlib.Path(args.out, "out.hmmer")) as hmmfile:
        for record in SearchIO.parse(hmmfile, "hmmer3-text"):
            hits = record.hits
            for hit in hits:
                hmmverif[hit.id][record.id.split("_")[1]].append(
                    record.id.split("_")[2]
                )
    hmmfile.close()

    if not args.nofilter:
        # Clear hmmverif from unwanted sequences
        msg("Filtering out artifacts identified by HMMs")
        hmmverif = clear_dict(hmmverif)
        msg("Done filtering")

    # Get list of wanted sequences
    goodhmmseq = list(hmmverif.keys())

    # Second itteration over output file to get evalues and hsps
    with open(pathlib.Path(args.out, "out.hmmer")) as hmmfile:
        for record in SearchIO.parse(hmmfile, "hmmer3-text"):
            hits = record.hits
            for hit in hits:
                for hsp in hit.hsps:
                    if hit.id in goodhmmseq:
                        hmmdict[hit.id][record.id.split("_")[1]].append(
                            f"{hit.evalue}#{hsp.hit_start}|{hsp.hit_end}"
                        )
    hmmfile.close()

    # Compute evalue by family

    hmmscore = hmm_threshold(hmmdict)

    # Predict sequence family according to HMM
    hmmfam = get_hmm_fam(hmmscore)

    msg("Done with HMM prediction")

    # PSSMs------------------------------------------------------------------
    msg(f"Running PSSM prediction using pfscan v{pfscan_match[0]}")

    # Run pfscan
    pssm_run = subprocess.run(
        [
            "pfscanV3",
            "--nthreads",
            str(cpus),
            "-o",
            "7",
            pathlib.Path(dbdir, "conodictor.pssm"),
            "-f",
            final_file,
        ],
        capture_output=True,
    )

    with open(pathlib.Path(args.out, "out.pssm"), "w") as po:
        po.write(pssm_run.stdout.decode("utf-8"))
    po.close()

    # Create two dict:
    #  - one to filter out grouped match without MAT profile
    #  - second to create dict for classification based on kept sequences
    pssmdict = defaultdict(list)
    pssmverif = defaultdict(lambda: defaultdict(list))
    pssmseq = defaultdict(lambda: defaultdict(list))

    # First itteration over output file for filtering
    with open(pathlib.Path(args.out, "out.pssm")) as pssmfile:
        rd = csv.reader(pssmfile, delimiter="\t")
        for row in rd:
            pssmverif[row[3]][(row[0].split("|")[0]).split("_")[1]].append(
                (row[0].split("|")[0]).split("_")[2]
            )
    pssmfile.close()

    if not args.nofilter:
        # Clear pssmverif from unwanted sequences
        msg("Filtering out artifacts identified by PSSMs")
        pssmverif = clear_dict(pssmverif)
        msg("Done filtering")

    # Get list of wanted sequences
    goodpssmseq = list(pssmverif.keys())

    # Second itteration over output file to get evalues and hsps
    with open(pathlib.Path(args.out, "out.pssm")) as pssmfile:
        rd = csv.reader(pssmfile, delimiter="\t")
        for row in rd:
            if row[3] in goodpssmseq:
                # Get also the matched sequence outputed by pfscan
                pssmdict[row[3]].append((row[0].split("|")[0]).split("_")[1])

                pssmseq[row[3]][(row[0].split("|")[0]).split("_")[1]].append(
                    f"{row[9].replace('-','').upper()}"
                    + f"#{(row[0].split('|')[0]).split('_')[2]}"
                )
    pssmfile.close()
    print(pssmseq)

    # Predict sequence family according to PSSM
    pssmfam = get_pssm_fam(pssmdict)

    msg("Done with PSSM predictions")

    # Writing output---------------------------------------------------------
    msg("Writing output")

    # Final families dict to store both predicted families
    finalfam = defaultdict(list)

    # Itterate through all submitted sequence to assign families
    for sid in seqids:
        if sid in hmmfam and sid in pssmfam:
            finalfam[sid].extend(
                [
                    hmmfam[sid],
                    pssmfam[sid],
                    definitive_prediction(hmmfam[sid], pssmfam[sid]),
                ]
            )
        elif sid in hmmfam and sid not in pssmfam:
            finalfam[sid].extend([hmmfam[sid], "UNKNOWN", hmmfam[sid]])
        elif sid in pssmfam and sid not in hmmfam:
            finalfam[sid].extend(["UNKNOWN", pssmfam[sid], pssmfam[sid]])
        else:
            finalfam[sid].extend(["UNKOWN", "UNKOWN", "UNKNOWN"])

    outfile = open(pathlib.Path(args.out, "summary.txt"), "a")

    # Get final sequences which has been classified
    uniq_final = {
        k: v
        for k, v in finalfam.items()
        if bool(set(v).intersection(["UNKNOWN", "UNKNOWN", "UNKNOWN"]))
        is False
    }

    # Enter "reads" mode
    if args.mlen:
        # --faa
        if args.faa:
            # Create dict of matched sequence for future access
            matched_sequences = defaultdict(list)

            for k, v in uniq_final.items():
                matched_sequences[k] = [
                    get_hmm_seq(hmmdict, k, infile),
                    get_pssm_seq(pssmseq, k, v[1]),
                ]

            # Write the fasta file
            with open(
                pathlib.Path(
                    args.out, f"{pathlib.Path(args.seqs).stem}_predicted.fa"
                ),
                "w",
            ) as faah:
                for k, v in matched_sequences.items():
                    faah.write(f">{k}_HMM\n{''.join(v[0])}\n")
                    faah.write(f">{k}_PSSM\n{''.join(v[1])}\n")
            faah.close()

            # Write the file of regions matched
            with open(
                pathlib.Path(
                    args.out, f"{pathlib.Path(args.seqs).stem}_regions.csv"
                ),
                "w",
            ) as freg:
                freg.write("sequence_id,signal,propeptide,mature\n")
                for k, v in matched_sequences.items():
                    freg.write(f"{k}_HMM,{', '.join(v[0])}\n")
                    freg.write(f"{k}_PSSM,{', '.join(v[1])}\n")
            freg.close()

        # write summary.txt file with sequence stats
        outfile.write(
            "sequence\tlength\tnum_cysteines\toccurence\t"
            + "hmm_pred\tpssm_pred\tdefinitive_pred\n"
        )

        if not args.all:
            for uk, uv in uniq_final.items():
                outfile.write(
                    f"{uk}\t"  # sequence id
                    + f"{get_stats(uk, infile)[0]}\t"  # sequence length
                    + f"{get_stats(uk, infile)[1]}\t"  # sequence nb cysteines
                    + f"{dupdata[uk]}\t"  # seq occ
                    + f"{uv[0]}\t"  # sequence HMM prediction
                    + f"{uv[1]}\t"  # sequence PSSM prediction
                    + f"{uv[2]}\n"  # sequence ConoDictor prediction
                )
            outfile.close()
        else:
            for k, v in finalfam.items():
                outfile.write(
                    f"{k}\t"
                    + f"{get_stats(k, infile)[0]}\t"
                    + f"{get_stats(k, infile)[1]}\t"
                    + f"{dupdata[k]}t"
                    + f"{v[0]}\t"
                    + f"{v[1]}\t"
                    + f"{v[2]}\n"
                )
            outfile.close()

    # "Transcriptome mode"
    else:
        # --faa
        if args.faa:
            # Create dict of matched sequence for future access
            matched_sequences = defaultdict(list)

            for k, v in uniq_final.items():
                matched_sequences[k] = [
                    get_hmm_seq(hmmdict, k, infile),
                    get_pssm_seq(pssmseq, k, v[1]),
                ]

            with open(
                pathlib.Path(
                    args.out, f"{pathlib.Path(args.seqs).stem}_predicted.fa"
                ),
                "w",
            ) as faah:
                for k, v in matched_sequences.items():
                    faah.write(f">{k}_HMM\n{''.join(v[0])}\n")
                    faah.write(f">{k}_PSSM\n{''.join(v[1])}\n")
            faah.close()

            # Write the file of regions matched
            with open(
                pathlib.Path(
                    args.out, f"{pathlib.Path(args.seqs).stem}_regions.csv"
                ),
                "w",
            ) as freg:
                for k, v in matched_sequences.items():
                    freg.write(f"{k}_HMM,{', '.join(v[0])}\n")
                    freg.write(f"{k}_PSSM,{', '.join(v[1])}\n")
            freg.close()

        # Open output file for writing
        outfile.write("sequence\thmm_pred\tpssm_pred\tdefinitive_pred\n")

        # Make reporting unclassified sequences optionnal
        if not args.all:

            # Write output
            for uk, uv in uniq_final.items():
                outfile.write(
                    f"{uk}\t"  # sequence id
                    + f"{uv[0]}\t"  # sequence HMM prediction
                    + f"{uv[1]}\t"  # sequence PSSM prediction
                    + f"{uv[2]}\n"  # sequence ConoDictor prediction
                )
            outfile.close()
        else:
            for k, v in finalfam.items():
                outfile.write(
                    f"{k}\t" + f"{v[0]}\t" + f"{v[1]}\t" + f"{v[2]}\n"
                )
            outfile.close()

    msg("Done with writing output.")

    # Finishing -------------------------------------------------------------
    # Cleaning around ...
    try:
        os.remove(pathlib.Path(args.out, "out.hmmer"))
        os.remove(pathlib.Path(args.out, "out.pssm"))
        os.remove(pathlib.Path(f"{args.seqs}.fxi"))
        os.remove(pathlib.Path(f"{inpath}.fxi"))
        os.remove(
            pathlib.Path(
                args.out, f"{pathlib.Path(args.seqs).stem}_allpep.fa"
            )
        )
        os.remove(pathlib.Path(args.out, "filtfa.fa"))
    except OSError:
        pass

    msg("Classification finished successfully.")
    msg("Creating donut plot")
    if args.mlen:
        donut_graph(6)
    else:
        donut_graph(3)
    msg("Done creating donut plot")
    msg(f"Check {args.out} folder for results")
    endtime = datetime.now()
    walltime = endtime - startime
    msg(f"Walltime used (hh:mm:ss.ms): {walltime}")
    if len(seqids) % 2:
        msg("Nice to have you. Share, enjoy and come back!")
    else:
        msg("Thanks you, come again.")


# Functions -----------------------------------------------------------------
def get_stats(id, infile):
    """
    get_stats return sequence length and number of cysteines in a sequences
    for a sequence id.

    :id: Input sequence id list.
    :infile: Fasta file to use to retrieve sequence.
    """
    stats = []

    # Sequence length
    stats.append(len(infile[id]))
    # Number of cysteines in sequence
    stats.append(infile[id].seq.count("C"))

    return stats


def get_dup_seqs(infile, idslist, mnoc):
    """
    get_dup_seqs search provided fasta file for duplicate sequence.
    Return sequence ids of duplicate sequences.

    :infile: Input fasta file to use for search
    :idslist: Sequence ids list to consider
    :mnoc: Minimum number of occurence wanted
    """

    dupid = {}
    flipped = defaultdict(set)
    seqdict = defaultdict()
    for id in idslist:
        seqdict[id] = infile[id].seq

    flipped = _flip_dict(seqdict)

    # The flipped dict is a dict of list
    # like: dict = {"ATCT": [id1, id2], "GCTA": [id4, id5]}
    # returning only the first element of the value
    # let us consider only one occurence of the sequence
    # Therefore we will really only predict one sequence
    # which can have multiple occurence
    for v in flipped.values():
        if len(v) >= mnoc:
            dupid[v[0]] = len(v)

    return dupid


def _flip_dict(mydict):
    """
    Return a flipped dict of input dict
    """

    flipped_ = defaultdict(list)
    for k, v in mydict.items():
        if v not in flipped_:
            flipped_[v] = [k]
        else:
            flipped_[v].append(k)

    return flipped_


def clear_dict(hdict):
    """
    clear_dict filter out sequences without a MATURE HMM or PSSM profile
    matched by hmmsearch or pfscan. It return the input dict with sequences
    without MATURE profile match filtered out.

    :hdict: A dictionnary containing matching profiles names by family by
            sequence id.

    example: {'sp|P0C640|CT55_CONPL':
              defaultdict(<class 'list'>, {'A': ['MAT', 'SIG']}),
              'sp|Q1A3Q6|CT57_CONLT':
               defaultdict(<class 'list'>, {'T': ['MAT', 'PRO', 'SIG']})}

    Such dict is created with defaultdict(lambda: defaultdict(list))
    """
    remove = defaultdict(list)

    # Get list of seq without mature sequence match
    for k in hdict.keys():
        for a, b in hdict[k].items():
            if "MAT" not in b:
                remove[k].append(a)

    # Remove families without mature sequence match
    for k, v in remove.items():
        for x in v:
            del hdict[k][x]

    # Remove sequence with no match with mature sequence
    for k in remove.keys():
        if not hdict[k]:
            del hdict[k]

    return hdict


def donut_graph(ncol):
    """
    donut_graph make a donut graph from outputed stats of
    predicted sequences.
    """

    data = pd.read_table(pathlib.Path(args.out, "summary.txt"))
    plot_data = data[data.columns[ncol]].tolist()
    dtc = Counter(plot_data)
    labels = [
        f"{k1}: {v1}"
        for k1, v1 in sorted(dtc.items())
        if not k1.startswith("CONFLICT")
    ]
    values = [
        x for k2, x in sorted(dtc.items()) if not k2.startswith("CONFLICT")
    ]

    # White circle
    _, ax = plt.subplots(figsize=(8, 5), subplot_kw=dict(aspect="equal"))
    wedges, _ = ax.pie(
        np.array(values).ravel(), wedgeprops=dict(width=0.5), startangle=-40
    )
    ax.legend(
        wedges, labels, loc="center left", bbox_to_anchor=(1, 0, 0.5, 1)
    )
    ax.set_title("ConoDictor Predictions")
    plt.text(-2.5, -1.5, f"Made with ConoDictor v{VERSION}")
    plt.savefig(
        pathlib.Path(args.out, "superfamilies_distribution.png"), dpi=300
    )


def definitive_prediction(hmmclass, pssmclass):
    """
    definitive_prediction gives definitive classification by
    combining HMM and PSSM classification.

    :hmmclass: HMM predicted family, required (string)
    :pssmclass: PSSM predicted family, required (string)
    """

    deffam = None

    if hmmclass == pssmclass:
        deffam = hmmclass
    elif "CONFLICT" in pssmclass and "CONFLICT" in hmmclass:
        fams_pssm = re.search("(?<=CONFLICT)(.*)and(.*)", pssmclass)
        fams_hmm = re.search("(?<=CONFLICT)(.*)and(.*)", hmmclass)
        deffam = f"CONFLICT {fams_pssm.group(1)}, {fams_pssm.group(2)},"
        +f" {fams_hmm.group(1)}, and {fams_hmm.group(2)}"
    elif "CONFLICT" in pssmclass and "CONFLICT" not in hmmclass:
        deffam = hmmclass
    elif "CONFLICT" in hmmclass and "CONFLICT" not in pssmclass:
        deffam = pssmclass
    elif pssmclass != hmmclass:
        deffam = f"CONFLICT {hmmclass} and {pssmclass}"

    return deffam


def isdnaorproteins(s):
    """
    isdnaorproteins test if input sequence is DNA or proteins.

    :s: input sequence
    """

    dna = "ATCG"
    prot = "ABCDEFGHIKLMNPQRSTVWYZ*"
    stype = ""

    if all(i in dna for i in s):
        stype = "DNA"
    elif all(i in prot for i in s):
        stype = "protein"
    else:
        stype = "unknown"

    return stype


def get_pssm_fam(mdict):
    """
    get_pssm_fam return the family with the highest number of
    occurence in PSSM profile match recorded as list for each
    sequence id.

    >>> my_dict = {ID1: ['A', 'A', 'B', 'M'], ID2: ['M', 'P', 'O1', 'O1']}
    >>> get_pssm_fam(my_dict)
    {ID1: 'A', ID2: 'O1'}

    :mdict: Dictionnary, required (dict)
    """

    fam = ""
    pssmfam = {}
    for key in mdict.keys():
        x = Counter(mdict[key])
        # Take the top 2 item with highest count in list
        possible_fam = x.most_common(2)

        if len(possible_fam) == 1:
            fam = possible_fam[0][0]
        elif len(possible_fam) > 1:
            if possible_fam[0][1] == possible_fam[1][1]:
                fam = (
                    f"CONFLICT {possible_fam[0][0]} and {possible_fam[1][0]}"
                )
            elif possible_fam[0][1] > possible_fam[1][1]:
                fam = possible_fam[0][0]
            else:
                fam = possible_fam[1][0]

        pssmfam[key] = fam

    return pssmfam


def hmm_threshold(mdict):
    """
    hmm_threshold calculate evalue by family for each sequence
    and return a dict with the evalue for each family.

    :mdict: Dictionnary, required (dict)
    """

    score = defaultdict(dict)
    for key in mdict.keys():
        for k, v in mdict[key].items():
            # v has the format evalue|hsp_start#hsp_end
            score[key][k] = reduce(
                mul, [Decimal(x.split("#")[0]) for x in v], 1
            )

    return score


def pssm_sorter_func(x):
    seq, reg = x.split("#")
    return reg


def get_pssm_seq(pdict, id, pclass):
    """
    get_pssm_seq returns sequence matched by PSSM profile.
    It take as input the pssmdict created from pfscan output and
    the id of the sequence wanted along with the final PSSM classification.

    :pdict: PSSM output file created from pfscan output.
    :id: sequence id to get matched sequence.
    :pclass: PSSM classification of desired sequence.
    """

    pseq = []

    for k in pdict.keys():
        for x, j in pdict[k].items():
            if k == id and x == pclass:
                # sort the regions to have them in the order sig, pro, mat
                # sig, pro, mat are ordered in reverse order: S > P > M
                v = sorted(
                    j,
                    key=pssm_sorter_func,
                    reverse=True,
                )
                pseq.extend([x.split("#")[0] for x in v])

    return pseq


def get_hmm_seq(mydict, mykey, file):
    """
    get_hmm_seq returns sequence matched by HMM profile.
    It take as input the hmmdict created from hmmsearch output and
    the id of the sequence to retrieve sequence from. It takes also
    the original fasta file to retrieve sequence from using sequence index.

    :mydict: Hmmdict created from hmmsearch output
    :mykey: Sequence id
    :file: input original file
    """

    hsp_id = {}
    hspseq = []
    ml = []

    for k in mydict.keys():
        for mv in mydict[k].values():
            if k == mykey:
                for x in mv:
                    a = x.split("#")[1]
                    ml.extend(a.split("|"))
                    fi = [int(b) for b in ml]
                hsp_id[k] = fi

    for k, v in hsp_id.items():
        v = sorted(v)
        hspseq = [(file[k].seq)[v[0] : v[-1]]]  # noqa

    return hspseq


def get_hmm_fam(mdict):
    """
    get_hmm_fam get sequence family from hmm dictionnary.

    :mdict: Dictionnary of evalues by families.
    """

    conofam = ""
    seqfam = {}
    for key in mdict.keys():
        two_smallest = nsmallest(2, mdict[key].values())

        if len(two_smallest) == 1:
            conofam = next(iter(mdict[key]))
        elif two_smallest[0] * 100 != two_smallest[1]:
            conofam = list(mdict[key].keys())[
                list(mdict[key].values()).index(two_smallest[0])
            ]
        elif two_smallest[0] * 100 == two_smallest[1]:
            fam1 = list(mdict[key].keys())[
                list(mdict[key].values()).index(two_smallest[0])
            ]
            fam2 = list(mdict[key].keys())[
                list(mdict[key].values()).index(two_smallest[1])
            ]
            conofam = f"CONFLICT {fam1} and {fam2}"

        seqfam[key] = conofam

    return seqfam


def print_install_tool(tool):
    """
    print_install_tool print useful installation
    instruction for required tools.
    """

    if tool == "hmmsearch":
        msg(f"{tool} not found. Please visit https://hmmer3.org.")
    elif tool == "pfscanV3":
        msg(
            f"{tool} not found. Please visit"
            + "https://github.com/sib-swiss/pftools3."
        )

    sys.exit(1)


def msg(text):
    """
    msg produce nice message and info output on terminal.

    :text: Message to print to STDOUT.
    """

    t = datetime.now().strftime("%H:%M:%S")
    line = f"[{t}] {text}"
    if not args.quiet:
        print(line, file=sys.stderr)


def _translate_seq(seq):
    """
    _translate_seq translate DNA sequence to proteins in the six frames.

    :seq: DNA sequence to translate.
    """

    seqlist = []
    # frame 1
    seqlist.append(translate(seq))
    # frame 2
    seqlist.append(translate(seq[1:]))
    # frame 3
    seqlist.append(translate(seq[2:]))
    # frame 4
    seqlist.append(translate(reverse_complement(seq)))
    # frame 5
    seqlist.append(translate(reverse_complement(seq)[1:]))
    # frame 6
    seqlist.append(translate(reverse_complement(seq)[2:]))

    return seqlist


def do_translation(infile, outfile, sw=60):
    """
    do_translation translate a DNA fasta file into proteins
    fasta file.

    :infile: Input DNA fasta file.
    :outfile: Output file.
    :sw: Sequence width. Default: 60.
    """

    seqin = pyfastx.Fasta(infile)
    with open(pathlib.Path(f"{outfile}_allpep.fa"), "w") as protfile:
        for sequence in seqin:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                protseq = _translate_seq(sequence.seq)
                for idx, frame in enumerate(protseq):
                    # Rule E203 from flacke8 check for extraneous whitespace
                    # before a colon. But black follow PEP8 rules.
                    # A PR is open to resolve this issue:
                    # https://github.com/PyCQA/pycodestyle/pull/914
                    seq_letters = [
                        frame[i : i + sw]  # noqa: E203
                        for i in range(0, len(frame), sw)
                    ]
                    nl = "\n"
                    protfile.write(
                        f">{sequence.name}_frame={idx + 1}\n"
                        + f"{nl.join(map(str, seq_letters))}\n"
                    )


def exception_handler(
    exception_type, exception, traceback, debug_hook=sys.excepthook
):
    """
    exception_handler remove default debug info and traceback
    from python output on command line. Use program --debug
    option to re-enable default behaviour.
    """

    if args.debug:
        debug_hook(exception_type, exception, traceback)
    else:
        msg(f"{exception_type.__name__}, {exception}")


sys.excepthook = exception_handler

if __name__ == "__main__":
    main()
